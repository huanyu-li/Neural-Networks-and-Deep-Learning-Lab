{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huanyu Li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech Tagging with Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task in this assignment is to implement a simple part-of-speech tagger based on recurrent neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-speech (POS) tagging is the task of labelling words (tokens) with [parts of speech](https://en.wikipedia.org/wiki/Part_of_speech). To give an example, consider the  sentence *Parker hates parsnips*. In this sentence, the word *Parker* should be labelled as a proper noun (a noun that is the name of a person), *hates* should be labelled as a verb, and *parsnips* should be labelled as a (common) noun. Part-of-speech tagging is an essential ingredient of many state-of-the-art natural language understanding systems.\n",
    "\n",
    "Part-of-speech tagging can be cast as a supervised machine learning problem where the gold-standard data consists of sentences whose words have been manually annotated with parts of speech. For the present assignment you will be using a corpus built over the source material of the [English Web Treebank](https://catalog.ldc.upenn.edu/ldc2012t13), consisting of approximately 16,000&nbsp;sentences with 254,000&nbsp;tokens. The corpus has been released by the [Universal Dependencies Project](http://universaldependencies.org).\n",
    "\n",
    "To make it easier to compare systems, the gold-standard data has been split into three parts: training, development (validation), and test. The following code uses three functions from the helper module `utils` (provided with this assignment) to load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the training data: 12543\n",
      "Number of sentences in the development data: 2002\n",
      "Number of sentences in the test data: 2077\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "training_data = list(utils.read_training_data())\n",
    "print('Number of sentences in the training data: {}'.format(len(training_data)))\n",
    "\n",
    "development_data = list(utils.read_development_data())\n",
    "print('Number of sentences in the development data: {}'.format(len(development_data)))\n",
    "\n",
    "test_data = list(utils.read_test_data())\n",
    "print('Number of sentences in the test data: {}'.format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a Python perspective, each of the data sets is a list of what we shall refer to as *tagged sentences*. A tagged sentence, in turn, is a list of pairs $(w,t)$, where $w$ is a word token and $t$ is the word&rsquo;s POS tag. Here is an example from the training data to show you how this looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'There', b'PRON'),\n",
       " (b'has', b'AUX'),\n",
       " (b'been', b'VERB'),\n",
       " (b'talk', b'NOUN'),\n",
       " (b'that', b'SCONJ'),\n",
       " (b'the', b'DET'),\n",
       " (b'night', b'NOUN'),\n",
       " (b'curfew', b'NOUN'),\n",
       " (b'might', b'AUX'),\n",
       " (b'be', b'AUX'),\n",
       " (b'implemented', b'VERB'),\n",
       " (b'again', b'ADV'),\n",
       " (b'.', b'PUNCT')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see part-of-speech tags such as `VERB` for verb, `NOUN` for noun, and `ADV` for adverb. If you are interested in learning more about the tag set used in the gold-standard data, you can have a look at the documentation of the [Universal POS tags](http://universaldependencies.org/u/pos/all.html). However, you do not need to understand the meaning of the POS tags to solve this assignment; you can simply treat them as labels drawn from a finite set of alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task in this assignment is to build a part-of-speech tagger based on a recurrent neural network architecture, to train this tagger on the provided training data, and to evaluate its performance on the test data. To tune the hyperparameters of the network, you can use the provided development (validation) data.\n",
    "\n",
    "### Network architecture\n",
    "\n",
    "The proposed network architecture for your tagger is a sequential model with three layers, illustrated below: an embedding, a bidirectional LSTM, and a softmax layer. The embedding turns word indexes (integers representing words) into fixed-size dense vectors which are then fed into the bidirectional LSTM. The output of the LSTM at each position of the sentence is passed to a softmax layer which predicts the POS tag for the word at that position.\n",
    "\n",
    "![System architecture](architecture.png)\n",
    "\n",
    "To implement the network architecture, you will use [Keras](https://keras.io), a high-level neural network library for Python. Keras comes with an extensive online documentation, and reading the relevant parts of this documentation will be essential when working on this assignment. We suggest to start with the tutorial [Getting started with the Keras Sequential model](https://keras.io/getting-started/sequential-model-guide/). We also suggest to have a look at concrete examples, such as  [imdb_lstm.py](https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py).\n",
    "\n",
    "### Pre-processing the data\n",
    "\n",
    "Before you can start to implement the network architecture as such, you will have to bring the tagged sentences from the gold-standard data into a form that can be used with the network. At its core, this involves encoding each word and each tag as an index into a finite set (a non-negative integer), which can be done for example via a Python dictionary. Here is some code to illustrate the basic idea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the training data: 19672\n",
      "Index of the word \"hates\": 4579\n"
     ]
    }
   ],
   "source": [
    "# Construct a simple index for words\n",
    "w2i = dict()\n",
    "for tagged_sentence in training_data:\n",
    "    for word, tag in tagged_sentence:\n",
    "        if word not in w2i:\n",
    "            w2i[word] = len(w2i)    # assign next available index\n",
    "\n",
    "print('Number of unique words in the training data: {}'.format(len(w2i)))\n",
    "print('Index of the word \"hates\": {}'.format(w2i[b'hates']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have indexes for the words and the tags, you can construct the input and the gold-standard output tensor required to train the network.\n",
    "\n",
    "**Constructing the input tensor.** The input tensor should be of shape $(N, n)$ where $N$ is the total number of sentences in the training data and $n$ is the length of the longest sentence. Note that Keras requires all sequences in an input tensor to have the same length, which means that you will have to pad all sequences to that length. You can use the helper function `pad_sequences` for this, which by default will front-pad sequences with the value&nbsp;0. It is essential then that you do not use this special padding value as the index of actual words.\n",
    "\n",
    "**Constructing the gold-standard output tensor.** The gold-standard output tensor should be of shape $(N, n, T)$ where $T$ is the number of unique tags in the training data, plus one to cater for the special padding value. The additional dimension corresponds to the fact that the softmax layer of the network will output one $T$-dimensional vector for each position of an input sentence. To construct the gold-standard version of this vector, you can use the helper function `to_categorical`, which will produce a &lsquo;one-hot vector&rsquo; for a given tag index.\n",
    "\n",
    "### Constructing the network\n",
    "\n",
    "To implement the network architecture, you need to find and instantiate the relevant building blocks from the Keras library. Note that Keras layers support a large number of optional parameters; use the default values unless you have a good reason not to. Two mandatory parameters that you will have to specify are the dimensionality of the embedding and the dimensionality of the output of the LSTM layer. The following values are reasonable starting points:\n",
    "\n",
    "* dimensionality of the embedding: 100\n",
    "* dimensionality of the output of the bidirectional LSTM layer: 100\n",
    "\n",
    "You will also have to choose an appropriate loss function. For training we recommend the Adam optimiser.\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "The last problem that you will have to solve is to write code to evaluate the trained tagger. The most widely-used evaluation measure for part-of-speech tagging is per-word accuracy, which is the percentage of words to which the tagger assigns the correct tag (according to the gold standard). Implementing this metric should be straightforward. However, make sure that you remove (or ignore) the special padding value when you compute the tagging accuracy.\n",
    "\n",
    "**The performance goal for this assignment is to build a tagger that achieves a development set accuracy of at least 90%.**\n",
    "\n",
    "**Unknown words.** One problem that you will encounter during evaluation is that the development data contains words that you did not see (and did not add to your index) during training. The simplest solution to this problem is to reserve a special index for &lsquo;the unknown word&rsquo; which the network can use whenever it encounters an unknown word. When you go for this strategy, the size of your index will be equal to the number of unique words in the training data plus&nbsp;2 &ndash; one extra for the unknown word, and one for the padding symbol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skeleton code\n",
    "\n",
    "The following skeleton code provides you with a starting point for your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM, Bidirectional, Embedding, TimeDistributed\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical, plot_model\n",
    "import keras.backend as K\n",
    "\n",
    "#import os\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "class Tagger(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.training_w2i = dict()\n",
    "        self.training_t2i = dict()\n",
    "        self.max_len = 0\n",
    "\n",
    "    def train(self, training_data):\n",
    "        # Pre-process the training data\n",
    "        # Construct the network, add layers, compile, and fit\n",
    "        self.preprocess(training_data)\n",
    "        self.max_len = len(max(training_data, key = len))\n",
    "        input_sequences = self.construct_input_tensors(training_data)\n",
    "        categorical_output = self.construct_output_tensors(training_data)\n",
    "        self.model = self.construct_net()\n",
    "        self.model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy', self.mean_pred])\n",
    "        self.model.fit(input_sequences, categorical_output, batch_size = 32, epochs = 6)\n",
    "\n",
    "    def preprocess(self, training_data):\n",
    "        for tagged_sentence in training_data:\n",
    "            for word, tag in tagged_sentence:\n",
    "                if word not in self.training_w2i:\n",
    "                    self.training_w2i[word] = len(self.training_w2i) + 1    # assign next available index\n",
    "                if tag not in self.training_t2i:\n",
    "                    self.training_t2i[tag] = len(self.training_t2i) + 1\n",
    "        self.training_w2i['-UNKNOWN-'] = len(self.training_w2i) + 1\n",
    "   \n",
    "    def construct_input_tensors(self, data):\n",
    "        input_tensor = list()\n",
    "        for tagged_sentence in data:\n",
    "            word_tensor = list()\n",
    "            for word, tag in tagged_sentence:\n",
    "                if word not in self.training_w2i.keys():\n",
    "                    word_tensor.append(self.training_w2i['-UNKNOWN-'])\n",
    "                else:\n",
    "                    word_tensor.append(self.training_w2i[word])\n",
    "            input_tensor.append(word_tensor)\n",
    "        input_sequences = sequence.pad_sequences(input_tensor, maxlen = self.max_len)\n",
    "        return input_sequences\n",
    "    \n",
    "    def construct_output_tensors(self, data):\n",
    "        output_tensor = list()\n",
    "        for tagged_sentence in data:\n",
    "            tag_tensor = list()\n",
    "            for word, tag in tagged_sentence:\n",
    "                tag_tensor.append(self.training_t2i[tag])\n",
    "            output_tensor.append(tag_tensor)\n",
    "        output_sequences = sequence.pad_sequences(output_tensor, maxlen = self.max_len)\n",
    "        return to_categorical(output_sequences)\n",
    "    \n",
    "    #https://keras.io/metrics/#custom-metrics\n",
    "    def mean_pred(self, y_true, y_pred):\n",
    "        #true_tag_tensor: [1,2,3,0,0,0]\n",
    "        #pred_tag_tensor: [4,2,3,0,0,0]\n",
    "        #last 3 are paddings\n",
    "        true_tag_tensor = K.argmax(y_true, axis = -1)\n",
    "        pred_tag_tensor = K.argmax(y_pred, axis = -1)\n",
    "        #sequence_tag = [1,1,1,0,0,0]\n",
    "        sequence_tag = K.cast(K.not_equal(pred_tag_tensor, 0), 'int64')\n",
    "        #matches = [0,1,1,1,1,1]\n",
    "        matches = K.cast(K.equal(true_tag_tensor, pred_tag_tensor), 'int64')\n",
    "        #cleaned_matches = [0,1,1,0,0,0]\n",
    "        cleaned_matches = matches * sequence_tag\n",
    "        acc = K.cast(K.sum(cleaned_matches), 'float64') / (K.cast(K.sum(sequence_tag), 'float64') + K.epsilon())\n",
    "        return acc\n",
    "    \n",
    "    def construct_net(self):\n",
    "        #+1 because of padding, did not store padding in training_w2i and training_t2i\n",
    "        embedding_input_dim = len(self.training_w2i) + 1\n",
    "        embedding_output_dim = 100\n",
    "        dense_input_dim = len(self.training_t2i) + 1\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(embedding_input_dim, embedding_output_dim))\n",
    "        model.add(Bidirectional(LSTM(100, return_sequences = True)))\n",
    "        model.add(TimeDistributed(Dense(dense_input_dim, activation = 'softmax')))\n",
    "        #plot_model(model, to_file='model.png')\n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "    def predict(self, data):\n",
    "        data_input = self.construct_input_tensors(data)\n",
    "        prediction = self.model.predict(data_input)\n",
    "        return prediction\n",
    "    \n",
    "    def evaluate(self, gold_data):\n",
    "        gold_input = self.construct_input_tensors(gold_data)\n",
    "        gold_output = self.construct_output_tensors(gold_data)\n",
    "        prediction = self.model.predict(gold_input)\n",
    "        return K.eval(self.mean_pred(gold_output, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how the tagger is supposed to be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         1967400   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 200)         160800    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 18)          3618      \n",
      "=================================================================\n",
      "Total params: 2,131,818\n",
      "Trainable params: 2,131,818\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "12543/12543 [==============================] - 145s 12ms/step - loss: 0.3062 - accuracy: 0.9217 - mean_pred: 0.2847\n",
      "Epoch 2/6\n",
      "12543/12543 [==============================] - 123s 10ms/step - loss: 0.0596 - accuracy: 0.9837 - mean_pred: 0.8473\n",
      "Epoch 3/6\n",
      "12543/12543 [==============================] - 114s 9ms/step - loss: 0.0214 - accuracy: 0.9943 - mean_pred: 0.9442\n",
      "Epoch 4/6\n",
      "12543/12543 [==============================] - 116s 9ms/step - loss: 0.0137 - accuracy: 0.9961 - mean_pred: 0.9617\n",
      "Epoch 5/6\n",
      "12543/12543 [==============================] - 117s 9ms/step - loss: 0.0104 - accuracy: 0.9969 - mean_pred: 0.9701\n",
      "Epoch 6/6\n",
      "12543/12543 [==============================] - 127s 10ms/step - loss: 0.0082 - accuracy: 0.9976 - mean_pred: 0.9767\n"
     ]
    }
   ],
   "source": [
    "tagger = Tagger()\n",
    "tagger.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on development data: 90.3%\n"
     ]
    }
   ],
   "source": [
    "acc = tagger.evaluate(development_data)\n",
    "print('Accuracy on development data: {:.1%}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Submit this assignment by emailing this notebook to Marco. Your notebook should include all your code, and should be runnable by Marco without further modification. It should also include a short text with your reflections on this assignment. What did you find particularly surprising or hard? What have you learned from this assignment? You can paste your text into the box below. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Reflections\n",
    "\n",
    "In the beginning, the accuracy of the development data was extremely high. I thought it was not correct, especially after I checked the predicted result for some samples, then I saw the result was not correct. Then I realized it was because of the padding zeros in the tensors. It took me some time to write the customed metric function.  As I started to do the lab before the lab session and didn't find there is a way to set MASK_ZERO as True to avoid writing my own customed metric for the evaluation. I have to write the customed metric mean_pred to ignore the padding zeros first, then calculate the accuracy. In the end, the network can achieve accuracy in 90.5% after trained with 32 batch size and six epochs. \n",
    "\n",
    "What I have learned from this assignment is, how to use some basic functions from Keras to build a neural network for this particular question. What is a little bit not easy to do is to figure out meanings for some default parameters of the classes in Keras. \n",
    "Considering the POS lab I did in TDDE09 (the data is the same, I think), the best performance of the multi-class perceptron can reach 87%. After some feature engineering work, the performance reaches 90%, which is close to the result of the neural network in this lab. It can be explained the former method takes limited features of the sequence even if it uses feature engineering, while the latter RNN based neural network considers the whole sequence. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
